{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Optimisation \n",
    "\n",
    "Least squares optimisation problems are those in which the __objective function__ is a quadratic function of the parameters to be optimised. The solutions to such problems can be computed analytically with linear algebra and have an intuitive interpretation in __Euclidean geometry__. \n",
    "\n",
    "## 1.1  Least squares regression \n",
    "\n",
    "Suppose we have a set of measurements of an unknown dependent variable, $y_n$, measured for a set of known values $x_n$. Suppose we believe that the measurements are proportional, but are corrupted by random measurement errors $\\epsilon_n$:\n",
    "\n",
    "\\begin{equation*}\n",
    "y_n = \\beta x_n + \\epsilon_n\n",
    "\\end{equation*}\n",
    "\n",
    "__Least squares regression__ aims at finding the value of $\\beta$ which minimises the __sum of squared errors__:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\beta} \\sum_{n=1}^{N} (y_n - \\beta x_n)^2\n",
    "\\end{equation*}\n",
    "\n",
    "Graphically, we are trying to find the __slope of the line__ through the origin that __best fits__ the measurements.\n",
    "\n",
    "<center>\n",
    "<img src='img/LS_plot.png'width=\"400\">\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "If we describe the set of measurments as vectors, $\\vec{y}$ and $\\vec{x}$, we are trying to find:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\beta} \\left\\|{\\vec{y} - \\beta \\vec{x}}\\right\\|^2\n",
    "\\end{equation*}\n",
    "\n",
    "We can consider __two different ways__ of finding the solution.\n",
    "\n",
    "### Method 1: Calculus \n",
    "\n",
    "The error expression is quadratic, so it has a minimum. We can set its derivative with respect to $\\beta$ equal to zero:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial}{\\partial \\beta} \\sum_{n=1}^{N} (y_n - \\beta x_n)^2 = 0 \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{n=1}^{N} 2 \\; (y_n - \\beta x_n) (-x_n) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{n=1}^{N} (-y_n x_n + \\beta {x_n}^2) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "The sum of $y_n x_n$ over all $n$ is simply the __dot product__ of $\\vec{y}$ with $\\vec{x}$. The sum of ${x_n}^2$ over all $n$ is the __determinant__ of $x^2$. This gives us:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{x} \\cdot \\vec{y} + \\beta \\; \\left\\|x\\right\\|^2 = 0\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta = \\frac {\\vec{x}\\vec{y}^T \\;}{\\left\\|x\\right\\|^2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "### Method 2:  Geometry\n",
    "\n",
    "We can also consider the problem from the perspective of the data vectors in $N$-dimensional space. We seek a scale factor $\\beta$ which scales the vector $\\vec{x}$ to be as close as possible to $\\vec{y}$. \n",
    "\n",
    "This point is where the projection of $\\vec{y}$ onto the line in the direction of $\\vec{x}$. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_{opt} \\vec{x} = (\\vec{y}^T \\hat{x}) \\hat{x} = \\frac{\\vec{y}^T \\vec{x}^2}{\\left\\|x\\right\\|^2}\n",
    "\\end{equation*}\n",
    "\n",
    "Solving for $\\beta_{opt}$ yields:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_{opt} = \\frac{\\vec{x} \\vec{y}^T} {\\left\\|x\\right\\|^2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "## 1.2  Multiple regression \n",
    "\n",
    "__Multiple regression__ is used when we believe the data is best explained as a weighted sum of __more than one explanatory variable.__ \n",
    "\n",
    "For example, we may believe the data are proportional to a set of known $x_n$s plus a constant, or that they are best fit by a third-order polynomial. These situations can be handled using __least squares regression__ as long as:\n",
    "\n",
    "- what we are fitting to the data is a weighted sum of _known_ explanatory variables\n",
    "- the error is expressed as a _sum of squared errors_ \n",
    "\n",
    "Suppose we have an $N$-dimensional data vector $\\vec{y}$ which can be explained with $M$ explanatory variables. The $m$th variable is a vector, $\\vec{x_m}$, whose elements are the values meant to explain each corresponding element of the data vector. \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{y} = \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} , \\; \\;\n",
    "\\vec{x}_1 = \n",
    "\\begin{bmatrix}\n",
    "x_{11} \\\\\n",
    "x_{12} \\\\\n",
    "\\vdots \\\\\n",
    "x_{1N} \\\\\n",
    "\\end{bmatrix} , \\; \\; \n",
    "\\vec{x}_2 = \n",
    "\\begin{bmatrix}\n",
    "x_{21} \\\\\n",
    "x_{22} \\\\\n",
    "\\vdots \\\\\n",
    "x_{2N} \\\\\n",
    "\\end{bmatrix} , \\; \\cdots, \\; \n",
    "\\vec{x}_M = \n",
    "\\begin{bmatrix}\n",
    "x_{M1} \\\\\n",
    "x_{M2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Each explanatory vector has a corresponding weight, $\\beta_m$. We are looking for the set of $\\beta$ s such that the weighted sum of the explanatory variables best approximates the data.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{y} \\approx \\beta_1 \\vec{x_1} + \\beta_2 \\vec{x_2} + ... + \\beta_M \\vec{x_M}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\beta_m}{\\left\\|\\vec{y} - \\sum_{m=1}^{M} \\beta_m \\vec{x}_m \\right\\|^2}\n",
    "\\end{equation*}\n",
    "\n",
    "We can form a matrix $X$ whose columns contain the explanatory vectors, and a vector $\\vec{\\beta}$ containing the $\\beta$s :\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{21} & \\dots & x_{M1} \\\\\n",
    "x_{12} & x_{22} & \\dots & x_{M2} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "x_{1N} & x_{2N} & \\dots & x_{MN} \\\\\n",
    "\\end{bmatrix} \\; \\; , \\; \\;\n",
    "\\vec \\beta = \n",
    "\\begin{bmatrix}\n",
    "\\beta_{1} \\\\\n",
    "\\beta_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_{N} \\\\\n",
    "\\end{bmatrix} \\; \\;\n",
    "\\end{equation*}\n",
    "\n",
    "We can then express the squared error more compactly as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\vec \\beta}{\\left\\|\\vec{y} - X \\vec\\beta \\right\\|^2}\n",
    "\\end{equation*}\n",
    "\n",
    "The vector $X \\vec \\beta$ is a weighted sum of the explanatory variables, which means it lies __somewhere in the subspace__ spanned by the columns of $X$. The vector $\\vec {y}$ may lie outside of that subspace. We seek a vector in the $X$ subspace that is as close as possible to the data vector $\\vec{y}$.\n",
    "\n",
    "<center>\n",
    "<img src='img/multiple_regression.png'width=\"400\">\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The smallest error vector, as before, must be __perpendicular__ to the $X$ subspace, and therefore to _each_ of the explanatory vectors. In other words, the $X$ matrix dotted with the error vector must equal the zero vector.\n",
    "\n",
    "\\begin{equation*}\n",
    "X^T \\cdot (\\vec{y} - X \\vec{\\beta}_{opt}) = \\vec{0}\n",
    "\\end{equation*}\n",
    "\n",
    "Solving for $\\vec{\\beta}$ gives:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\beta}_{opt} = (X^TX)^{-1} X^T \\vec{y}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### Understanding the solution in the orthogonal case \n",
    "\n",
    "We can gain intuition into the solution by considering the case where the columns of the explanatory matrix $X$ are __orthogonal__ to each other. In this case, $X^TX$ will be a _diagonal matrix_, and its inverse will have diagonal elements $ {1} / {\\left\\|\\vec{x}_m\\right\\|^2}$. The product of this inverse matrix with $X^T$ is thus a matrix whose rows contain the __original explanatory variables divided by their squared norms__. \n",
    "\n",
    "Each element of the solution $\\vec \\beta_{opt}$ is then the __inner product of the data vector with the corresponding explanatory variable divided by its squared norm__. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_{opt_{n}} = y_n \\cdot \\frac {\\vec{x}_m}{\\left\\| \\vec{x}_m \\right\\|^2}\n",
    "\\end{equation*}\n",
    "\n",
    "Each $\\vec{x}_m$ is rescaled to explain the part of $\\vec{y}$ that __lies along its own direction__, and the solution for each explanatory variable is not affected by the others.\n",
    "\n",
    "\n",
    "### Understanding the solution in the non-orthogonal case\n",
    "\n",
    "\n",
    "If the columns of $X$ are not orthogonal, the solution is best understood by using the __singular value decomposition__ (SVD).\n",
    "\n",
    "We can re-write the matrix $X$ as $USV^T$. We need to get $USV^T$ to approximate $\\vec{y}$.  The optimisation problem is now:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\vec{\\beta}} \\left\\|\\vec{y} - USV^T \\vec{\\beta}\\right\\|^2\n",
    "\\end{equation*}\n",
    "\n",
    "We can express the error vector in a more useful coordinate system by multiplying it by the matrix $U^T$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left\\|\\vec{y} - USV^T \\vec{\\beta}\\right\\|^2 = \\left\\|U^T (\\vec{y} - USV^T \\vec{\\beta})\\right\\|^2 = \\left\\|U^T \\vec{y} - SV^T \\vec{\\beta})\\right\\|^2\n",
    "\\end{equation*}\n",
    "\n",
    "Let's now define the data vector $\\vec{y}^*$ to equal $ U^T \\vec{y}$, and the parameter vector $\\vec \\beta^*$ to equal $V^T \\vec{\\beta}$. We can __re-write our error function__ and solve the modified problem:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\vec \\beta^*} \\left\\|\\vec{y}^* - S \\vec\\beta^*\\right\\|^2\n",
    "\\end{equation*}\n",
    "\n",
    "The matrix $S$ is diagonal, and has $M$ columns. So the $m$th element of the vector $S \\vec \\beta^*$ is of the form $S_{mm} \\beta^*_m$ for the first $M$ elements. The remaining $N - M$ elements are zero. The total error $E(\\vec \\beta^*)$ is the sum of the squared differences between the elements of $\\vec{y}$ and the elements of $S \\vec \\beta^*$ , which we can write as:\n",
    "\n",
    "\\begin{equation*}\n",
    "E(\\vec \\beta^*) = \\left\\|\\vec{y}^* - S \\vec\\beta^*\\right\\|^2 = \\sum_{m=1}^{M}(y_m^* - S_{mm} \\beta_m^*)^2 \\; + \\sum_{m = M + 1}^{N} (y_m^*)^2\n",
    "\\end{equation*}\n",
    "\n",
    "Each term of the first sum can be __set to zero__ by choosing $\\beta_m^* = {y_m^*} / {S_{mm}}$.\n",
    "\n",
    "However, the terms in the second sum are __unaffected by the choice of__ $\\beta_m^*$, and thus __cannot be eliminated.__ So the minimum value of the error is equal to the sum of the squared values of the last $N − M$ elements of $\\vec{y}^*$.\n",
    "\n",
    "We can place the elements of our error vector, $\\beta_m^* = {y_m^*} / {S_{mm}}$, into the vector defined by:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\beta}^*_{opt} = S^\\# \\vec{y}^* \n",
    "\\end{equation*}\n",
    "\n",
    "where $S^\\#$ is a diagonal matrix whose $m$th diagonal element is $1 / S_{mm}$. \n",
    "\n",
    "Finally, we can transform our solution back into the original parameter space:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\beta}_{opt} = V \\vec{\\beta}^*_{opt} = V S^\\# \\vec{y}^*  = V S^\\# U^T \\vec{y} \n",
    "\\end{equation*}\n",
    "\n",
    "### Robustness\n",
    "\n",
    "Least squares regression is notoriously __non-robust__ to outliers. Single bad datapoints have a strong influence on the solution. To avoid this, one can use a __robust error metric__ $d$ in place of the squared error:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_\\vec\\beta \\sum_{n} d (y_n - X_n \\vec{\\beta})\n",
    "\\end{equation*}\n",
    "\n",
    "A common choice is the __Lorentzian__ function:\n",
    "\n",
    "\\begin{equation*}\n",
    "d(e_n) = log (1 + (e_n / \\sigma)^2 )\n",
    "\\end{equation*}\n",
    "\n",
    "which we see plotted here alongside the squared error function. The two functions are similar for small errors, but the Lorentzian gives a smaller penalty to large errors. \n",
    "\n",
    "\n",
    "<center>\n",
    "<img src='img/lorentzian.png'width=\"400\">\n",
    "</center>\n",
    "\n",
    "\n",
    "Using such a function normally means we can no longer get an __analytical solution__ to the problem. Instead, we have to use an __iterative numerical algorithm__ (e.g. gradient descent) to search the parameter space for a local minimum. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 1.3  Constrained least squares \n",
    "\n",
    "Sometimes the prior assumption is that the $\\vec{\\beta}$s cannot simply be anything. Two common constraints are:\n",
    "\n",
    "- __Linear__ - $\\beta$ must lie on a line \n",
    "- __Quadratic__ - $\\beta$ cannot be arbitrarily large\n",
    "\n",
    "For example, suppose we want the solution vector $\\beta$ to correspond to a __weighted average__ of the explanatory variables - in other words, its elements must sum to one. For this, we'd use a __constraint vector__ $\\vec{c}$ filled with ones. The optimisation problem is now:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_\\vec{\\beta} \\left\\| \\vec{y} - X \\vec{\\beta} \\right\\|^2 \\; \\ni \\; \\vec{c}^T \\vec{\\beta} = 1 \n",
    "\\end{equation*}\n",
    "\n",
    "To solve, we again replace the __regressor matrix__ with its SVD, $X = USV^T$, then eliminate the orthogonal matrices $U$ and $V$ from the problem by re-parameterising:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_\\vec{\\beta^*} \\left\\| \\vec{y}^* - S \\vec{\\beta}^* \\right\\|^2 \\; \\ni \\; \\vec{c}^{*T} \\vec{\\beta}^* = 1 \n",
    "\\end{equation*}\n",
    "\n",
    "where $\\vec{y}^*  = U^TY$, $\\vec \\beta^* = V^T \\vec{\\beta}$ and $\\vec c^* = V^T \\vec{c}$. \n",
    "\n",
    "Finally, we re-write the problem as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_\\vec{\\beta^{**}} \\left\\| \\vec{y}^{**} - \\vec{\\beta}^{**} \\right\\|^2 \\; \\ni \\; \\vec{c}^{**T} \\vec{\\beta}^{**} = 1 \n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "- $\\vec{y}^{**}$ is the top $M$ elements of $\\vec{y}^*$, where $M$ is the number of regressors\n",
    "- $\\vec \\beta^{**} = S^* \\vec \\beta^{*}$\n",
    "- $\\vec c^{**} = S^\\# \\vec c^{*}$\n",
    "- $S^*$ is the square matrix containing the top $M$ rows of $S$\n",
    "- $S^\\#$ is the inverse of $S^*$ \n",
    "\n",
    "\n",
    "This transformed problem can be visualised best in two dimensions. We want the vector $\\vec \\beta^{**}$ that lies on the line defined by $\\vec{c}^{**T} \\vec{\\beta}^{**} = 1$, and is __closest__ to  $\\vec{y}^{**}$. \n",
    "\n",
    "The solution is the __projection of the vector__  $\\vec{y}^{**}$ onto the __hyperplane perpendicular to__ $\\vec{c}^{**T}$. So we can write that $\\vec{\\beta}^{**}_{opt}$ plus some multiple of its perpendicular line, $\\vec{c}^{**}$, is equal to $\\vec{y}^{**}$. We therefore have $ \\vec{\\beta}^{**}_{opt} = \\vec{y}^{**} - \\alpha \\vec{c}^{**}$ , and, since $\\vec{c}^{**}$ and $ \\vec{\\beta}^{**}_{opt}$ must be perpendicular, we must solve for a value of $\\alpha$ that satisfies the constraint:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{c}^{**T} (\\vec{y}^{**} - \\alpha \\vec{c}^{**}) = 1\n",
    "\\end{equation*}\n",
    "\n",
    "The solution is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha_{opt} = \\frac{\\vec{c}^{**T} \\vec{y}^{**} - 1} {\\vec{c}^{**T}\\vec{c}^{**}}\n",
    "\\end{equation*}\n",
    "\n",
    "Finally, transforming back to the original parameter space gives:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec \\beta_{opt} = V S^\\# \\vec\\beta^{**}_{opt} = VS^\\#(\\vec{y}^{**} - \\alpha_{opt} \\vec{c}^{**})\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "## 1.4 Total least squares regression\n",
    "\n",
    "In __classical least squares regression__, errors are defined as the squared distance between the dependent variable and a weighted combination of the independent variables. When each measurement is a __vector of values__, there is no clear distinction between \"dependent\" and \"independent\" variables. It makes more sense to measure errors as the __squared perpendicular distance to the line__. \n",
    "\n",
    "In most cases, we want to fit $N$-dimensional data with a subspace of dimensionality $N - 1$. We can express the __sum of squared distances__ of the data to this $N - 1$ dimensional space as the __sum of squared projections onto a unit vector__ $\\hat{u}$ that is __perpendicular__ to the space.\n",
    "\n",
    "The optimisation problem can thus be expressed as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\vec{u}} \\left\\|M \\vec{u}\\right\\|^2 \\; \\ni \\; \\left\\|\\vec{u}\\right\\|^2 = 1\n",
    "\\end{equation*}\n",
    "\n",
    "where $M$ is a matrix containing the data vectors in its rows.\n",
    "\n",
    "We can find the solution by performing an SVD on the matrix $M$. We let $M = USV^T$. Then:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\left\\|M\\vec{u}\\right\\|^2 & {}={} \\vec{u}^T M^T M \\vec{u} \\\\\n",
    "& = \\vec{u}^T VS^T U^T USV^T \\vec{u}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Since $V$ is an orthogonal matrix, we can substitute the vector $\\vec{v} = V^T \\vec{u}$, which is simply a rotation of the vector $\\vec{u}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\vec{v}} \\vec{v}^T S^T S \\vec{v} \\; \\ni \\; \\left\\|\\vec{v}\\right\\|^2 = 1\n",
    "\\end{equation*}\n",
    "\n",
    "The matrix $S^TS$ is square and diagonal, with entries $s^2_n$. The matrix $\\vec{v}^T \\vec{v}$ is orthogonal with components $v^2_n$. So the expression being minimised is a weighted sum of the components of $\\vec{v}$ which must be greater or equal to the square of the smallest (last) singular value, $s_N$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\vec{v}^T S^T S \\vec{v} & {}={} \\sum_{n} s^2_n v^2_n \\\\\n",
    "& \\geq \\sum_{n} s^2_N v^2_n \\\\\n",
    "& = s^2_N \\sum_{n}v^2_n \\\\\n",
    "& = s^2_N \\left\\|\\vec{v}\\right\\|^2 \\\\\n",
    "& = s^2_N\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "On the condition that $\\left\\|\\vec{v}\\right\\|^2 = 1$, in the final step.\n",
    "\n",
    "\n",
    "So $\\vec{v}^T S^T S \\vec{v}$ is always greater or equal to $s^2_N$. The solution is therefore minimised when $\\vec{v}$ is the unit vector associated with the smallest singular value, the $N$th row of $S$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v}_{opt} = \\hat{e}_N = [0 \\; 0 \\; \\cdots \\; 0 \\; 1]^T\n",
    "\\end{equation*}\n",
    "\n",
    "We can transform this solution back to the original coordinate system to get a solution for $\\vec{u}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\vec{u}_{opt} & {}={} V \\vec{v}_{opt}\\\\\n",
    "& = V \\hat{e}_N \\\\\n",
    "& = \\vec{V}_N \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "which is the $N$th column of the matrix $V$. So, the __minimum value of the expression occurs__ when we set $\\vec{u}$ equal to the __column of $V$ associated with the minimum singular value__.  \n",
    "\n",
    "## 1.5  Relationship to eigenvector analysis\n",
    "\n",
    "Any __square, symmetric real matrix__ can be factorised as:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = V \\Lambda V^T\n",
    "\\end{equation*}\n",
    "\n",
    "where $V$ is a matrix whose columns are the __orthonormal eigenvectors of $A$,__ and $\\Lambda$ is a diagonal matrix containing the __eigenvalues__, $s^2_n$.\n",
    "\n",
    "The total least squares problem can be restated in terms of eigenvectors by noting a relationship between the __SVD of $M$__ and the __eigenvector decomposition of $M^TM$.__ The problem always involves minimising:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\left\\|M\\vec{u}\\right\\|^2 & {}={} \\vec{u}^T M^T M \\vec{u} \\\\\n",
    "& = \\vec{u}^T V \\Lambda V^T \\vec{u} \\\\\n",
    "& = \\vec{u}^T (VS^T SV^T \\vec{u})\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "The __eigenvectors__ are the columns of V; the __eigenvalues__ are the singular values of $\\Lambda$.\n",
    "\n",
    "Consider the parenthesised expression. When $\\vec{u} = \\vec{u}_n$, the $n$th column of $V$, this becomes:\n",
    " \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "VS^T SV^T \\vec{u}_n & {}={} VS^TS \\hat{e}_n \\\\\n",
    "& = V s^2_n \\hat{e}_n \\\\\n",
    "& = s^2_n \\vec{v}_n\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\vec{e}_n$ is the $n$the standard basis vector. So $\\vec{v}_n$ are the eigenvectors of $M^TM$, with eigenvalues $\\lambda_n = s^2_n$. \n",
    "\n",
    "So the eigenvector analysis of $M^TM$ is similar to the SVD of $M$, but in some ways more useful.\n",
    "\n",
    "\n",
    "## 1.6  Principle Components Analysis\n",
    "\n",
    "The shape of a data cloud can be summarised with an __ellipsoid__ using the following procedure:\n",
    "\n",
    "1. Subtract the __mean__ of all data points to recentre around the origin \n",
    "\n",
    "2. Assemble the centred data vectors in rows of a matrix, $M$\n",
    "\n",
    "3. Compute the SVD of $M$,  $USV^T$, or the eigenvectors of $M^TM$, $V\\Lambda V^T$\n",
    "\n",
    "4. The columns of $V$ are the __principle components__ of the ellipsoid, and the diagonal elements of $\\Lambda$, $s_k$, are the corresponding sizes of the ellipsoid \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Vector operations\n",
    "\n",
    "## Norm \n",
    "\n",
    "The __norm__ or __magnitude__ of a vector is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left\\|\\vec{v}\\right\\| = \\sqrt{\\sum_n{v_n^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "This corresponds to the __length__ of the vector. \n",
    "\n",
    "## Unit vectors\n",
    "\n",
    "A __unit vector__ is a vector of length one.\n",
    "\n",
    "Any vector can be rescaled to have __unit length__ by dividing by its __norm__. In other words, a vector may be factorised into a product of its norm and a unit vector. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{v} = \\frac{\\vec{v}}{\\left\\|\\vec{v}\\right\\|} \\qquad \\qquad\n",
    "\\vec{v} = {\\left\\|\\vec{v}\\right\\|}\\:\\hat{v}\n",
    "\\end{equation*}\n",
    "\n",
    "## Dot product\n",
    "\n",
    "The __inner product__, or __dot product,__ of two vectors is the sum of the pairwise product of their components:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "c \\\\\n",
    "\\end{bmatrix} \n",
    "\\bullet\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "ax \\\\\n",
    "+ \\\\\n",
    "by \\\\\n",
    "+ \\\\\n",
    "cz \\\\\n",
    "\\end{bmatrix} \n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v} \\cdot \\vec{w} \\equiv \\sum_{n}v_n w_n\n",
    "\\end{equation*}\n",
    "\n",
    "Geometrically, this equates to:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v} \\cdot \\vec{w} \\equiv {\\left\\|\\vec{v}\\right\\|} \\; {\\left\\|\\vec{w}\\right\\|} \\cos{(\\phi_{vw})}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\phi_{vw}$ is the __angle__ between the two vectors.\n",
    "\n",
    "This means the dot product is 0 when $\\left\\|\\vec{v}\\right\\|$, $\\left\\|\\vec{w}\\right\\|$ or $\\cos{\\phi}$ is 0. \n",
    "\n",
    "If we let $b$ equal the length of $\\vec{w}$ projected onto $\\vec{v}$, the cosine of the angle of the vectors can be defined as $\\frac{b}{\\left\\|\\vec{w}\\right\\|}$. This gives us:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v} \\cdot \\vec{w} \\equiv \n",
    "\\frac{b}{\\left\\|\\vec{w}\\right\\|}{\\left\\|\\vec{v}\\right\\|} \\; {\\left\\|\\vec{w}\\right\\|}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src='img/dot_product.png'width=\"400\">\n",
    "</center>\n",
    "\n",
    "In other words, the dot product of $\\vec{v}$ with $\\vec{w}$ can be visualised as the __component of $\\vec{w}$ projected onto $\\vec{v}$__, scaled by the length of $\\vec{v}$. If $\\vec{w}$ is the __unit vector__ $\\hat{u}$, the it is simply the __projection__ of $\\vec{v}$ onto the line in the direction of $\\hat{u}$. \n",
    "\n",
    "\n",
    "<center>\n",
    "<img src='img/dot_product_unit.png'width=\"400\">\n",
    "</center>\n",
    "\n",
    "\n",
    "As a summary, the dot product of:\n",
    "- two __perpendicular__ vectors = 0\n",
    "- two __parallel__ vectors = the product of their norms\n",
    "- a __vector with itself__ = the square of its norm\n",
    "\n",
    "## Vector spaces\n",
    "\n",
    "A __vector space__ is a collection of vectors closed under linear combination. All vector spaces include the __zero vector__. \n",
    "\n",
    "A __subspace__ is a vector space lying within another vector space - as in, a 2D plane in a 3D space.\n",
    "\n",
    "A set of vectors __spans__ a vector space if one can write any vector in that space as a linear combination of the set. A set can be redundant - for example, if two of the vectors are scaled copies of each other. \n",
    "\n",
    "A set of vectors $\\{ \\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\}$ is __linearly independent__ if and only if the only solution to the equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{n} \\alpha_n \\vec{v}_n = 0\n",
    "\\end{equation*}\n",
    "\n",
    "is $\\alpha_n = 0$ for all $n$.\n",
    "\n",
    "<center>\n",
    "<img src='img/vector_spaces.png'width=\"350\">\n",
    "</center>\n",
    "\n",
    "## Basis vectors\n",
    "\n",
    "The __basis__ of a vector space is its __linearly independent spanning set__. This means the minimum set of vectors required to span it. \n",
    "\n",
    "In general, the vector space $R^N$ requires a basis of size $N$.\n",
    "\n",
    "The __basis vectors__ define a set of coordinate axes for the space. They _need not be perpendicular_. \n",
    "\n",
    "The __standard basis__ is the set of __unit vectors__ that lie along the axes of the space:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{e}_1 = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} , \\;\n",
    "\\hat{e}_2 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} , \\; \\cdots \\;\n",
    "\\hat{e}_N = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "<center>\n",
    "<img src='img/basis_vector.png'width=\"400\">\n",
    "</center>\n",
    "\n",
    "## Vectors as operators\n",
    "\n",
    "It can be useful to interpret vectors as __operators__ applied to an input. For example, the dot product of a vector $\\vec{v}$ with the vector:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{w} =  \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{N} \\\\\n",
    "\\frac{1}{N} \\\\\n",
    "... \\\\\n",
    "\\frac{1}{N} \\\\\n",
    "\\end{bmatrix} \n",
    "\\end{equation*}\n",
    "\n",
    "gives the __average__ of the components of $\\vec{v}$ . \n",
    "\n",
    "\n",
    "Likewise, the dot product with the vector:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{w} =  \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "-1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} \n",
    "\\end{equation*}\n",
    "\n",
    "computes the __local difference__ between two components of $\\vec{v}$ . \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "# 1.2  Linear Systems & Matrices \n",
    "\n",
    "## Linear systems\n",
    "\n",
    "A __linear system__ $S$ transforms vectors in one vector space into those of another vector space, obeying the __principle of superposition__: \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "S \\{ a \\vec{v} + b \\vec{w} \\} = aS\\{ \\vec{v} \\} + bS\\{ \\vec{w} \\}\n",
    "\\end{equation*}\n",
    "\n",
    "This means the __system response__ to any linear combination of vectors is equal to the linear combination of the responses to each of the vectors alone. \n",
    "\n",
    "In other words:\n",
    "- multiplying by the scaling factor\n",
    "- adding together\n",
    "- then applying the linear transformation S\n",
    "\n",
    "is equivalent to:\n",
    "- applying the linear transformation S\n",
    "- adding together\n",
    "- then multiplying by the scaling factor\n",
    "\n",
    "We can write any vector as a sum of its __standard basis vectors__ multiplied by a respective __weight__:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v} = v_1 \\hat{e}_1 + v_2 \\hat{e}_2 + \\cdots + v_n \\hat{e}_N\n",
    "\\end{equation*}\n",
    "\n",
    "Using the definition of linear systems, we can write:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "S \\{ \\hat{v} \\} & {}={} S \\{ v_1 \\hat{e}_1 + v_2 \\hat{e}_2 + \\cdots + v_n \\hat{e}_N \\} \\\\\n",
    "& = v_1 S \\{ \\hat{e}_1 \\} + v_2 S \\{ \\hat{e}_2 \\} + \\cdots + v_n S \\{ \\hat{e}_N \\}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "This means the __response of a linear system__ is a __weighted sum__ of the responses to each of the __standard basis vectors__. We can call these responses __response vectors__. The system is __fully characterised__ by this set of response vectors.\n",
    "\n",
    "<center>\n",
    "<img src='img/linear_system.png'width=\"450\">\n",
    "</center>\n",
    "\n",
    "We can put the column vectors $S \\{ \\hat{e}_1 \\}, S \\{ \\hat{e}_2 \\} ... etc $ corresponding to the __responses__ to each basis vector into a __matrix__. This matrix is a complete representation of the linear system. \n",
    "\n",
    "\\begin{equation*}\n",
    "S = \n",
    "\\begin{bmatrix}\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "S \\{ \\hat{e}_1 \\} & S \\{ \\hat{e}_2 \\} & \\cdots &  S \\{ \\hat{e}_n \\} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The __system response__ is a __weighted sum__ of the __columns__ of the matrix.\n",
    "\n",
    "If we call the elements of the matrix $S_{rc}$, with $r$ indicating the __rows__ and $c$ the __columns__, the response $\\vec{w}$ of the system to an input vector $\\vec{v}$ has components:\n",
    "\n",
    "\\begin{equation*}\n",
    "w_r = \\sum_{c} S_{rc} v_c\n",
    "\\end{equation*}\n",
    "\n",
    "In other words, each component of the output vector is a __dot product__ of the corresponding __row__ of the matrix with the input vector. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{w} = S\\cdot\\vec{v}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "\\vdots \\\\\n",
    "w_n \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "S \\{ \\hat{e}_1 \\} & S \\{ \\hat{e}_2 \\} & \\cdots &  S \\{ \\hat{e}_n \\} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\end{bmatrix}\n",
    "\\bullet\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "v_3 \\\\\n",
    "\\vdots \\\\\n",
    "v_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "For this to work, the __number of columns__ in the matrix $S$ must equal the __number of rows__ in the input vector $\\vec{v}$. \n",
    "\n",
    "## Matrix transpose\n",
    "\n",
    "The __transpose__ of a matrix is the same matrix but with the __rows and columns swapped__. \n",
    "\n",
    "\\begin{equation*}\n",
    "(S_{nm})^T = S_{mn}\n",
    "\\end{equation*}\n",
    "\n",
    "A __symmetric matrix__ is a square matrix that is equal to its transpose. \n",
    "\n",
    "## Diagonal matrices \n",
    "\n",
    "A __diagonal matrix__ is one for which the only non-zero components can be along the diagonal.\n",
    "\n",
    "\\begin{equation*}\n",
    "D = \\left( \\begin{array}{ccccc}\n",
    "d_{11} &  &  &  &  \\\\\n",
    " & d_{22} &  &  &  \\\\\n",
    " &  & d_{33} & &  \\\\\n",
    " &  &  & \\ddots &  \\\\\n",
    " &  &  &  & d_{nn} \\\\\n",
    "\\end{array} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "In general, the operation performed by diagonal matrices is to __stretch or compress__ vector spaces. The $n$th axis of the space is stretched or compressed by the amount in the $n$th diagonal element, $S_{nn}$. \n",
    "\n",
    "\n",
    "The diagonal matrix for which the diagonal components are all 1s is called the __identity matrix,__ $I$. As such, the identity matrix leaves the vector space untransformed. For example:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "I_4 = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### Inverse of diagonal matrices\n",
    "\n",
    "The inverse of a __square diagonal matrix__ is a matrix in which each diagonal component is the scalar inverse of the original stretch factor: $\\begin{equation*}\n",
    "D_{nn}\\;^{-1} = \\frac{1}{D_{nn}}\n",
    "\\end{equation*}$.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "D^{-1} = \\left( \\begin{array}{ccccc}\n",
    "\\frac{1}{d_{11}} &  &  &  &  \\\\\n",
    " & \\frac{1}{d_{22}} &  &  &  \\\\\n",
    " &  & \\frac{1}{d_{33}} & &  \\\\\n",
    " &  &  & \\ddots &  \\\\\n",
    " &  &  &  & \\frac{1}{d_{nn}} \\\\\n",
    "\\end{array} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Applying the diagonal matrix $D$ and its inverse $D^{-1}$ in succession restore the original vector, so the matrix product of the two is the identity matrix. \n",
    "\n",
    "\\begin{equation*}\n",
    "D^{-1}D = I\n",
    "\\end{equation*}\n",
    "\n",
    "However, if an element of the diagonal is __zero__, it __cannot be inverted__. This is because, in multiplying a component of a vector space with 0, the associated axis is __annihilated__ during the transformation. \n",
    "\n",
    "### Nullspace and rank \n",
    "\n",
    "The set of axes \"annihilated\" by the matrix form the __nullspace__ of the matrix. In contrast, the set of axes the matrix can \"reach\" is the __column space__ of the matrix. Any matrix with __nonzero nullspace__ cannot be inverted.\n",
    "\n",
    "The __rank__ of a matrix is the dimensionality of the column space. \n",
    "\n",
    "A matrix is __full rank__ if its rank is equal to at least the smaller of its two dimensions.\n",
    "\n",
    "## Orthogonal matrices\n",
    "\n",
    "An __orthogonal matrix__ is a __square matrix__ for which:\n",
    "\n",
    "- every column is a _unit vector_\n",
    "- every pair of columns is _orthogonal_\n",
    "\n",
    "\n",
    "The __transpose__ of the orthogonal matrix $O$ multiplied by itself gives the identity matrix:\n",
    "\n",
    "\\begin{equation*}\n",
    "O^{T}O = I\n",
    "\\end{equation*}\n",
    "\n",
    "The operation performed by orthogonal matrices is a __generalised rotation__ vector spaces. It maps the original standard basis onto a new set of $N$ orthogonal axes. It does not stretch, compress or change the angle between two vectors in vector space.\n",
    "\n",
    "To give a few examples:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{bmatrix}$  Performs a __reflection__ across the x-axis.\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0\n",
    "\\end{bmatrix}$  Performs a __permutation__ of the coordinate axes.\n",
    "\n",
    "<br> \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "cos \\theta & -sin \\theta \\\\\n",
    "sin \\theta & cos \\theta\n",
    "\\end{bmatrix}$  Performs a __rotation__ by $\\theta$.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Inverse of orthogonal matrices\n",
    "\n",
    "The inverse of an orthogonal matrix is simply its __transpose__. Since $O$ corresponds to a generalised rotation of the space, $O^{T}$ corresponds to the same rotation in the opposite direction.\n",
    "\n",
    "\\begin{equation*}\n",
    "O^{T} = O^{-1}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "# 1.3  Singular Value Decomposition\n",
    "\n",
    "Any matrix $M$ can be decomposed into a product of three matrices:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{M}_{W \\times D} = \\underbrace{U}_{W \\times W} \\times \\underbrace{\\Sigma}_{W\\times D} \\times \\underbrace{V^{T}}_{D \\times D}\n",
    "\\end{equation}\n",
    "\n",
    "such that $U$ and $V$ are __orthogonal matrices__ and $\\Sigma$ is a __diagonal matrix__ with positive entries.\n",
    "\n",
    "The diagonal elements of $\\Sigma$ are called the __singular values__. \n",
    "\n",
    "This decomposition separates the action of $M$ into three components:\n",
    "\n",
    "- $V^{T}$ -  __rotation__ of vector space into new coordinate system\n",
    "- $\\Sigma$ - __scaling__ of axes of vector space\n",
    "- $U$ - __rotation__ of vector space into output coordinate system \n",
    "\n",
    "\n",
    "<center>\n",
    "<img src='img/SVD_1.png'width=\"400\">\n",
    "</center>\n",
    "\n",
    "The __nullspace__ of $M$ corresponds to the columns of $V$ which align with __zero singular values__. \n",
    "\n",
    "The __column space__ (also known as _range space_) of $M$ corresponds to the columns of $U$ which align with __non-zero singular values__.\n",
    "\n",
    "The matrix is __invertible__ if and only if the _number of non-zero singular values_ equals the _number of columns of M._\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "M = U \\Sigma V^T = \n",
    "\\begin{matrix}\n",
    "\\underbrace{\\left[\\begin{matrix} & \\\\ & \\\\ & \\\\ \\vec u_1 & \\dots & \\vec u_r \\\\ & \\\\ & \\\\ & \\\\ \\end{matrix}\\right.}& \n",
    "    \\underbrace{\\left.\\begin{matrix} & \\\\ & \\\\ & \\\\ \\vec u_{r+1} & \\dots &  \\vec u_m \\\\ & \\\\ & \\\\ & \\\\ \\end{matrix}\\right]}\\\\\n",
    "Col M & Nul M\n",
    "    \\end{matrix}\n",
    "    \\begin{bmatrix}\n",
    "      \\sigma_1 & & & & & \\\\\n",
    "      & \\sigma_2  & & &  & \\\\\n",
    "      & & \\ddots & & & \\\\\n",
    "      & & & \\sigma_r  & &  \\\\\n",
    "      & & & & 0 & &  \\\\\n",
    "      & & & & & \\ddots & \\\\\n",
    "      & & & & & & 0 \n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    \\vec v_1^T \\\\ \\dots \\\\ \\vec v_r^T \\\\ & \\\\\n",
    "    \\vec v_{r+1}^T \\\\ \\dots \\\\ \\vec v_n^T\n",
    "  \\end{bmatrix}\n",
    "  \\begin{matrix}\n",
    "    \\left.\\vphantom{\\begin{bmatrix}\n",
    "       \\vec v_1^T \\\\ \\vec v_2^T \\\\ \\dots \\\\ \\vec v_r^T \n",
    "       \\end{bmatrix}}\\right\\} Row M \\\\ \n",
    "    \\left.\\vphantom{\\begin{bmatrix}\n",
    "      \\vect v_{r+1}^T \\\\ \\dots \\\\ \\vect v_n^T \n",
    "    \\end{bmatrix}}\\right\\} Nul M\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "The __singular values__ in $\\sigma_1, \\sigma_2... etc$ are ordered according to size. The first singular value is the 'most significant'. This means we can choose a 'cut-off' and approximate the original matrix $M$ just using the first $n$ singular values.\n",
    "\n",
    "\\begin{equation*}\n",
    "M = \\sigma_1 \\vec u_1 \\vec v^{T}_1\n",
    "+ \\sigma_2 \\vec u_2 \\vec v^{T}_2 + \\dots + \\sigma_n \\vec u_n \\vec v^{T}_n\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "The SVD is __guaranteed to exist__ for any matrix. It is also __unique,__ outside of variations such as permuting or negating the components. \n",
    "\n",
    "The transformation of $M$ on a vector $\\vec{x}$ can be seen as a weighted sum of the outer products of $U$ and $V^{T}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "M \\vec{x} = \\sum_{n} \\sigma_n (\\vec u_n \\vec v^{T}_n) \\vec{x}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
